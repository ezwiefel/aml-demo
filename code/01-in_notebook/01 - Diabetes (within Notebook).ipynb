{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Diabetes Regression Model In Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "install"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "install"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/\"\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print()\n",
    "print(f'Workspace name:\\t{ws.name}', \n",
    "      f'Azure region:\\t{ws.location}',\n",
    "      f'Subscription:\\t{ws.subscription_id}',\n",
    "      f'Resource group:\\t{ws.resource_group}',\n",
    "      sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data\n",
    "We will use the diabetes dataset for this experiement, a well-known small dataset that comes with scikit-learn.  This cell loads the dataset and splits it into random training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "print(load_diabetes()['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = pd.read_csv(os.path.join(DATA_PATH,\"diabetes.csv\"))\n",
    "\n",
    "diabetes_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diabetes_df.pop('target').values\n",
    "X = diabetes_df.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print (f\"Data contains {len(X_train)} training samples and {len(X_test)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Let's use scikit-learn to train a simple Ridge regression model.  We use AML to record interesting information about the model in an Experiment.  An Experiment contains a series of trials called Runs.  During this trial we use AML in the following way:\n",
    "* We access an experiment from our AML workspace by name, which will be created if it doesn't exist\n",
    "* We use `start_logging` to create a new run in this experiment\n",
    "* We use `run.log()` to record a parameter, alpha, and an accuracy measure - the Mean Squared Error (MSE) to the run.  We will be able to review and compare these measures in the Azure Portal at a later time.\n",
    "* We store the resulting model in the **outputs** directory, which is automatically captured by AML when the run is complete.\n",
    "* We use `run.complete()` to indicate that the run is over and results can be captured and finalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(y, y_hat):\n",
    "    resids = y - y_hat\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    sns.regplot(y, resids)\n",
    "    \n",
    "    plt.xlabel(\"Actual Value\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    \n",
    "    plt.close(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "local run",
     "outputs upload"
    ]
   },
   "outputs": [],
   "source": [
    "experiment_name = 'diabetes_regression'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "local run",
     "outputs upload"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a run object in the experiment\n",
    "with experiment.start_logging() as run:\n",
    "    # Log the algorithm parameter alpha to the run\n",
    "    alpha = 0.03\n",
    "    run.log('alpha', alpha)\n",
    "    run.log('model_type', 'Ridge')\n",
    "    \n",
    "    # Create, fit, and test the scikit-learn Ridge regression model\n",
    "    regression_model = Ridge(alpha=alpha)\n",
    "    regression_model.fit(X_train, y_train)\n",
    "    preds = regression_model.predict(X_test)\n",
    "\n",
    "    # Output the Mean Squared Error to the notebook and to the run\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "\n",
    "    print(f\"RMSE:\\t{np.round(rmse,4)}\",\n",
    "          f\"MAE:\\t{np.round(mae,4)}\",\n",
    "          f\"R2:\\t{np.round(r2,4)}\",\n",
    "          sep='\\n')\n",
    "    \n",
    "    # Log metrics to Azure ML\n",
    "    run.log('rmse', rmse)\n",
    "    run.log('mae', mae)\n",
    "    run.log('r2', r2)\n",
    "    \n",
    "    plt_fig = plot_residuals(y_test, preds)\n",
    "    \n",
    "    run.log_image(name='residuals', plot=plt_fig)\n",
    "    \n",
    "    # Save the model to the outputs directory for capture\n",
    "    joblib.dump(value=regression_model, filename='outputs/model.pkl')\n",
    "    \n",
    "plt_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we encapsulate the logic in a function, we can do a *simple* Hyperparameter sweep with a loop\n",
    "def train_ridge_regression(run, X_train, y_train, X_test, y_test, alpha=0.03, model_filename='model.pkl', show_output=False):\n",
    "    # Log the algorithm parameter alpha to the run\n",
    "    run.log('alpha', alpha)\n",
    "    run.log('model_type', 'Ridge')\n",
    "    \n",
    "    # Create, fit, and test the scikit-learn Ridge regression model\n",
    "    regression_model = Ridge(alpha=alpha)\n",
    "    regression_model.fit(X_train, y_train)\n",
    "    preds = regression_model.predict(X_test)\n",
    "\n",
    "    # Output the Mean Squared Error to the notebook and to the run\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    \n",
    "    if show_output:\n",
    "        print(f\"RMSE:\\t{np.round(rmse,4)}\",\n",
    "              f\"MAE:\\t{np.round(mae,4)}\",\n",
    "              f\"R2:\\t{np.round(r2,4)}\",\n",
    "              sep='\\n')\n",
    "    \n",
    "    # Log metrics to Azure ML\n",
    "    run.log('rmse', rmse)\n",
    "    run.log('mae', mae)\n",
    "    run.log('r2', r2)\n",
    "    \n",
    "    plt_fig = plot_residuals(y_test, preds)\n",
    "    \n",
    "    run.log_image(name='residuals', plot=plt_fig)\n",
    "    \n",
    "    # Save the model to the outputs directory for capture\n",
    "    joblib.dump(value=regression_model, filename=f'outputs/{model_filename}')\n",
    "    \n",
    "    return plt_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple parameter sweep\n",
    "Now let's take the same concept from above and modify the **alpha** parameter.  For each value of alpha we will create a run that will store metrics and the resulting model.  In the end we can use the captured run history to determine which model was the best for us to deploy. \n",
    "\n",
    "Note that by using `with experiment.start_logging() as run` AML will automatically call `run.complete()` at the end of each loop.\n",
    "\n",
    "This example also uses the **tqdm** library to provide a status bar feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"model.pkl\"\n",
    "\n",
    "# list of numbers from 0 to 1.0 with a 0.05 interval\n",
    "alphas = np.arange(0.0, 1.0, 0.05)\n",
    "\n",
    "# try a bunch of alpha values in a Linear Regression (Ridge) model\n",
    "for alpha in tqdm(alphas):\n",
    "    # create a bunch of runs, each train a model with a different alpha value\n",
    "    with experiment.start_logging() as run:\n",
    "        train_ridge_regression(run, X_train, y_train, X_test, y_test, alpha, model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing experiment results\n",
    "Similar to viewing the run, we can also view the entire experiment.  The experiment report view in the Azure portal lets us view all the runs in a table, and also allows us to customize charts.  This way, we can see how the alpha parameter impacts the quality of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's take a look at the experiment in Azure portal.\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn\n",
    "Repeat the above steps with another sklearn algorithm. \n",
    "\n",
    "Hint: Check out [Scikit-Learn.ensemble](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) or [Scikit-Learn.linear_model](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) for other algorithm to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_algo_name_here():\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best model \n",
    "Now that we've created many runs with different parameters, we need to determine which model is the best for deployment.  For this, we will iterate over the set of runs.  From each run we will take the *run id* using the `id` property, and examine the metrics by calling `run.get_metrics()`.  \n",
    "\n",
    "Since each run may be different, we do need to check if the run has the metric that we are looking for, in this case, **mse**.  To find the best run, we create a dictionary mapping the run id's to the metrics.\n",
    "\n",
    "Finally, we use the `tag` method to mark the best run to make it easier to find later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = {}\n",
    "run_metrics = {}\n",
    "\n",
    "# Create dictionaries containing the runs and the metrics for all runs containing the 'mse' metric\n",
    "for r in tqdm(experiment.get_runs()):\n",
    "    metrics = r.get_metrics()\n",
    "    if 'rmse' in metrics.keys():\n",
    "        runs[r.id] = r\n",
    "        run_metrics[r.id] = metrics\n",
    "\n",
    "# Find the run with the best (lowest) mean squared error and display the id and metrics\n",
    "best_run_id = min(run_metrics, key = lambda k: run_metrics[k]['rmse'])\n",
    "best_run = runs[best_run_id]\n",
    "print('Best run is:', best_run_id)\n",
    "print('Metrics:', run_metrics[best_run_id])\n",
    "\n",
    "# Tag the best run for identification later\n",
    "best_run.tag(\"Best Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Deploy\n",
    "Now that we have trained a set of models and identified the run containing the best model, we want to deploy the model for real time inferencing.  The process of deploying a model involves\n",
    "* registering a model in your workspace\n",
    "* creating a scoring file containing init and run methods\n",
    "* creating an environment dependency file describing packages necessary for your scoring file\n",
    "* creating a docker image containing a properly described environment, your model, and your scoring file\n",
    "* deploying that docker image as a web service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a model\n",
    "We have already identified which run contains the \"best model\" by our evaluation criteria.  Each run has a file structure associated with it that contains various files collected during the run.  Since a run can have many outputs we need to tell AML which file from those outputs represents the model that we want to use for our deployment.  We can use the `run.get_file_names()` method to list the files associated with the run, and then use the `run.register_model()` method to place the model in the workspace's model registry.\n",
    "\n",
    "When using `run.register_model()` we supply a `model_name` that is meaningful for our scenario and the `model_path` of the model relative to the run.  In this case, the model path is what is returned from `run.get_file_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "query history"
    ]
   },
   "outputs": [],
   "source": [
    "# View the files in the run\n",
    "for f in best_run.get_file_names():\n",
    "    print(f)\n",
    "    \n",
    "# Register the model with the workspace\n",
    "model = best_run.register_model(model_name='diabetes_regression_model', model_path='outputs/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model is registered, it is accessible from the list of models on the AML workspace.  If you register models with the same name multiple times, AML keeps a version history of those models for you.  The `Model.list()` lists all models in a workspace, and can be filtered by name, tags, or model properties.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "register model from history"
    ]
   },
   "outputs": [],
   "source": [
    "# Find all models called \"best_model\" and display their version numbers\n",
    "from azureml.core.model import Model\n",
    "models = Model.list(ws, name='diabetes_regression_model')\n",
    "for m in models:\n",
    "    print(m.name, m.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a scoring file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import Ridge\n",
    "from azureml.core.model import Model\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # note here \"best_model\" is the name of the model registered under the workspace\n",
    "    # this call should return the path to the model.pkl file on the local disk.\n",
    "    model_path = Model.get_model_path(model_name='diabetes_regression_model')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "# note you can pass in multiple rows for scoring\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        data = json.loads(raw_data)['data']\n",
    "        data = np.array(data)\n",
    "        result = model.predict(data)\n",
    "\n",
    "        # you can return any data type as long as it is JSON-serializable\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "from azureml.core.image import ContainerImage\n",
    "\n",
    "conda_file_path = './aml_config/condaEnv.yml'\n",
    "os.makedirs('./aml_config/')\n",
    "\n",
    "# Create an empty conda environment and add the scikit-learn package\n",
    "env = CondaDependencies()\n",
    "env.add_conda_package(\"scikit-learn\")\n",
    "\n",
    "# Display the environment\n",
    "print(env.serialize_to_string())\n",
    "\n",
    "# Write the environment to disk\n",
    "with open(conda_file_path,\"w\") as f:\n",
    "    f.write(env.serialize_to_string())\n",
    "\n",
    "# Create a configuration object indicating how our deployment container needs to be created\n",
    "image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n",
    "                                    runtime=\"python\", \n",
    "                                    conda_file=conda_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe your target compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={'disease': 'diabetes', 'target': 'blood_sugar'}, \n",
    "                                               description='Diabetes Regression Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy your webservice\n",
    "**Note:** The web service creation can take several minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "# Create the webservice using all of the precreated configurations and our best model\n",
    "service = Webservice.deploy_from_model(name='diabetes-svc',\n",
    "                                       deployment_config=aciconfig,\n",
    "                                       models=[model],\n",
    "                                       image_config=image_config,\n",
    "                                       workspace=ws)\n",
    "\n",
    "# Wait for the service deployment to complete while displaying log output\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test your webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# scrape the first row from the test set.\n",
    "test_samples = json.dumps({\"data\": X_test[0:1, :].tolist()})\n",
    "\n",
    "#score on our service\n",
    "service.run(input_data = test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows how you can send multiple rows to the webservice at once.  It then calculates the residuals - that is, the errors - by subtracting out the actual values from the results.  These residuals are used later to show a plotted result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "# score the entire test set.\n",
    "test_samples = json.dumps({'data': X_test.tolist()})\n",
    "\n",
    "result = service.run(input_data = test_samples)\n",
    "residual = result - y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell shows how you can use the `service.scoring_uri` property to access the HTTP endpoint of the service and call it using standard POST operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# use the first row from the test set again\n",
    "test_samples = json.dumps({\"data\": X_test[0:1, :].tolist()})\n",
    "\n",
    "# create the required header\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "# post the request to the service and display the result\n",
    "resp = requests.post(service.scoring_uri, test_samples, headers = headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the ACI instance to stop the compute and any associated billing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "deploy service",
     "aci"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "haining"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:azureml-sdk]",
   "language": "python",
   "name": "conda-env-azureml-sdk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
